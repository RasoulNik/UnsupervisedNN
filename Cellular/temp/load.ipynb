{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.1.0\n",
      "Number of GPUs available : 4\n",
      "Used GPU: 1. Memory growth: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Sat Mar 28 15:36:42 2020\n",
    "\n",
    "@author: nikbakht\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function\n",
    "#---------------------------------\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "host=socket.gethostname()\n",
    "if host=='N-20HEPF10AVE2':\n",
    "    pass\n",
    "else:\n",
    "    num_GPU = 1 # GPU  to use, can be 0, 1, 2 or 3\n",
    "    mem_growth = True\n",
    "    print('Tensorflow version: ', tf.__version__)\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    print('Number of GPUs available :', len(gpus))\n",
    "    tf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[num_GPU], mem_growth)\n",
    "    print('Used GPU: {}. Memory growth: {}'.format(num_GPU, mem_growth))\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "#import scipy.io as sio\n",
    "#import h5py\n",
    "#import pandas as pd\n",
    "from datetime import datetime\n",
    "# from Data_conv import Data\n",
    "from Data import Data\n",
    "from Plot_results import Plot\n",
    "\n",
    "# from UNNdebug import UNN\n",
    "from UNN import UNN\n",
    "from Loss import Loss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "#train_iterations = 100\n",
    "batch_size =1000\n",
    "# train_per_database=100\n",
    "# database_size=batch_size*train_per_database\n",
    "EPOCHS =int(10e3)\n",
    "Nuser=30\n",
    "Nap=30\n",
    "#Lambda=.001\n",
    "#alpha=1\n",
    "Id_save='2'\n",
    "save_model=1\n",
    "P_over_noise=120; # dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(obj,Dataobj,epochs,mode):\n",
    "    # TF board logs\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = './logs/' + current_time + '/train'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "    best_test_rate = -float('inf')\n",
    "    best_W = None\n",
    "    LR=np.logspace(-3,-4, num=epochs)\n",
    "    Xin,G_batch,_=Dataobj(100*batch_size)\n",
    "    SNR=np.power(10,P_over_noise/10)*G_batch\n",
    "    Xin=np.power(10,P_over_noise/10)*tf.linalg.diag_part(SNR)\n",
    "    Xin_av=np.mean(np.log(Xin),axis=0)\n",
    "    Xin_std=np.std(np.log(Xin),axis=0)\n",
    "    try:\n",
    "        for i in range(epochs):\n",
    "            LR_i=LR[i ]\n",
    "            optimizer = tf.keras.optimizers.Adam(LR_i)\n",
    "            xin,G_batch,_=Dataobj(batch_size)\n",
    "            SNR=np.power(10,P_over_noise/10)*G_batch\n",
    "            # xin=np.reshape(G_batch,[batch_size,-1])\n",
    "            xin=np.log(np.diagonal(SNR,axis1=1,axis2=2))\n",
    "            xin=(xin-Xin_av)/Xin_std\n",
    "            J=[]\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass.\n",
    "                cost,_ = obj(xin,SNR)\n",
    "                # Get gradients of loss wrt the weights.\n",
    "                gradients = tape.gradient(cost, obj.trainable_weights)\n",
    "                # Gradient clipping\n",
    "                # c_gradients,grad_norm = tf.clip_by_global_norm(gradients, 1.0)\n",
    "                # Update the weights of our linear layer.\n",
    "                optimizer.apply_gradients(zip(gradients, obj.trainable_weights))\n",
    "                J.append(cost.numpy())\n",
    "            \n",
    "    \n",
    "            if i % 50 == 0:\n",
    "                # test_rate=cost.numpy()[0]\n",
    "                test_rate=np.mean(J)\n",
    "#                bit2r.LR=bit2r.LR*.85\n",
    "                print('iter i=',i,'average cost is ', test_rate)\n",
    "                if test_rate > best_test_rate:\n",
    "                    best_test_rate = test_rate\n",
    "                best_W = obj.get_weights()\n",
    "                save_model(obj, 'models/'+mode+'UNN.mod')\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    tf.summary.scalar('test rate', test_rate, step=i)\n",
    "                    tf.summary.scalar('best test rate', best_test_rate, step=i)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    obj.set_weights(best_W)\n",
    "    return Xin_av,Xin_std\n",
    "\n",
    "def save_model(model, fn):\n",
    "    W = model.get_weights()\n",
    "    with open(fn, 'wb') as f:\n",
    "        pickle.dump(W, f)\n",
    "        \n",
    "def load_model(model, fn):\n",
    "    with open(fn, 'rb') as f:\n",
    "        W = pickle.load(f)\n",
    "    model.set_weights(W)\n",
    "def SINR(SNR,p,Nap,Nuser):\n",
    "    p=tf.exp(p)\n",
    "#    p=p+1e-5;\n",
    "    num=tf.zeros([p.shape[0],1], dtype='float64') \n",
    "    denom=tf.zeros(num.shape, dtype='float64') \n",
    "    SINR=tf.zeros([SNR.shape[0],Nuser], dtype='float64')\n",
    "    ta = tf.TensorArray(tf.float64, size=Nuser)\n",
    "\n",
    "    for k in range(Nuser):\n",
    "       num=tf.multiply(p[:,k],SNR[:,k,k])\n",
    "       Total=tf.multiply(p,SNR[:,k,:])\n",
    "       denom=1+tf.reduce_sum(Total,axis=1)-Total[:,k]\n",
    "       ta = ta.write(k,tf.divide(num,denom))\n",
    "\n",
    "    SINR=tf.transpose(ta.stack(),perm=[1,0])\n",
    "    return SINR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Data(Nuser)\n",
    "unn=UNN(Nap,Nuser)\n",
    "Xin_av,Xin_std=train(unn,data,EPOCHS,'x')\n",
    "#tensorboard --logdir ./logs --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------\n",
    "xin,G_batch,p_frac=data(batch_size)\n",
    "# xin=np.reshape(G_batch,[batch_size,-1])\n",
    "SNR=np.power(10,P_over_noise/10)*G_batch\n",
    "xin=np.log(np.diagonal(SNR,axis1=1,axis2=2))\n",
    "xin=(xin-Xin_av)/Xin_std\n",
    "SIR=SINR(SNR,unn.Network(xin),Nap,Nuser)\n",
    "plot=Plot()\n",
    "plot.cdfplot(SIR.numpy(),p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
